#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GetQuickeræ¨èé¡µé¢ä½œè€…ç»Ÿè®¡å·¥å…·
ä»æ¨èé¡µé¢æå–ä½œè€…ä¿¡æ¯ï¼Œè·å–æ¯ä¸ªä½œè€…çš„ç»Ÿè®¡æ•°æ®
"""

import requests
import json
import sys
import logging
import time
import pandas as pd
import re
from lxml import html
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin
from get_user_actions import get_all_user_actions, setup_logger


def get_recommended_page_content(logger: logging.Logger) -> Optional[str]:
    """
    è·å–æ¨èé¡µé¢HTMLå†…å®¹
    
    Args:
        logger (logging.Logger): æ—¥å¿—è®°å½•å™¨
    
    Returns:
        Optional[str]: é¡µé¢HTMLå†…å®¹ï¼Œå¤±è´¥è¿”å›None
    """
    url = "https://getquicker.net/Share/Recommended"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
        'Referer': 'https://getquicker.net/Share',
    }
    
    try:
        logger.debug(f"æ­£åœ¨è®¿é—®æ¨èé¡µé¢: {url}")
        response = requests.get(url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            logger.debug(f"âœ… æˆåŠŸè·å–æ¨èé¡µé¢ï¼çŠ¶æ€ç : {response.status_code}")
            return response.text
        else:
            logger.error(f"âŒ è·å–æ¨èé¡µé¢å¤±è´¥ï¼çŠ¶æ€ç : {response.status_code}")
            return None
            
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
        return None


def extract_authors_from_recommended(html_content: str, logger: logging.Logger) -> List[Dict[str, str]]:
    """
    ä»æ¨èé¡µé¢HTMLä¸­æå–ä½œè€…ä¿¡æ¯
    
    Args:
        html_content (str): é¡µé¢HTMLå†…å®¹
        logger (logging.Logger): æ—¥å¿—è®°å½•å™¨
    
    Returns:
        List[Dict[str, str]]: ä½œè€…ä¿¡æ¯åˆ—è¡¨
    """
    try:
        tree = html.fromstring(html_content)
        authors = []
        
        # æŸ¥æ‰¾æ‰€æœ‰ä½œè€…é“¾æ¥
        # æ ¹æ®é¡µé¢ç»“æ„ï¼Œä½œè€…é“¾æ¥é€šå¸¸åœ¨è¡¨æ ¼ä¸­çš„åˆ†äº«äººåˆ—
        author_links = tree.xpath('//a[contains(@href, "/User/")]')
        
        logger.info(f"æ‰¾åˆ° {len(author_links)} ä¸ªä½œè€…é“¾æ¥")
        
        for link in author_links:
            try:
                href = link.get('href', '')
                if '/User/' in href:
                    # æå–ç”¨æˆ·ID - åªå–æ•°å­—éƒ¨åˆ†ï¼Œå»æ‰ç”¨æˆ·å
                    full_id = href.split('/User/')[-1].split('?')[0].split('#')[0]
                    # å¦‚æœåŒ…å«ç”¨æˆ·åï¼ˆå¦‚ 113342/Ceaï¼‰ï¼Œåªå–æ•°å­—éƒ¨åˆ†
                    if '/' in full_id:
                        user_id = full_id.split('/')[0] + '-'  # æ·»åŠ è¿å­—ç¬¦
                    else:
                        user_id = full_id
                    author_name = link.text_content().strip()
                    
                    # é¿å…é‡å¤
                    if not any(author['user_id'] == user_id for author in authors):
                        authors.append({
                            'user_id': user_id,
                            'author_name': author_name,
                            'profile_url': f"https://getquicker.net{href}"
                        })
                        logger.debug(f"æå–ä½œè€…: {author_name} (ID: {user_id})")
                
            except Exception as e:
                logger.debug(f"å¤„ç†ä½œè€…é“¾æ¥æ—¶å‡ºé”™: {e}")
                continue
        
        logger.info(f"âœ… æˆåŠŸæå– {len(authors)} ä¸ªå”¯ä¸€ä½œè€…")
        return authors
        
    except Exception as e:
        logger.error(f"âŒ æå–ä½œè€…ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return []


def get_author_stats(author: Dict[str, str], logger: logging.Logger) -> Dict[str, any]:
    """
    è·å–å•ä¸ªä½œè€…çš„ç»Ÿè®¡æ•°æ®
    
    Args:
        author (Dict[str, str]): ä½œè€…ä¿¡æ¯
        logger (logging.Logger): æ—¥å¿—è®°å½•å™¨
    
    Returns:
        Dict[str, any]: ä½œè€…ç»Ÿè®¡æ•°æ®
    """
    try:
        logger.info(f"æ­£åœ¨è·å–ä½œè€… {author['author_name']} (ID: {author['user_id']}) çš„ç»Ÿè®¡æ•°æ®...")
        
        # ä½¿ç”¨ç°æœ‰çš„get_all_user_actionså‡½æ•°
        actions, stats = get_all_user_actions(author['user_id'], logger)
        
        # æ„å»ºä½œè€…ç»Ÿè®¡ä¿¡æ¯
        author_stats = {
            'user_id': author['user_id'],
            'author_name': author['author_name'],
            'profile_url': author['profile_url'],
            'total_actions': stats['total_actions'],
            'total_likes': stats['total_likes'],
            'total_downloads': stats['total_downloads'],
            'total_pages': stats['total_pages'],
            'avg_likes': stats['avg_likes'],
            'avg_downloads': stats['avg_downloads'],
            'extraction_time': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        logger.info(f"âœ… {author['author_name']} - åŠ¨ä½œæ•°: {stats['total_actions']}, æ€»ç‚¹èµ: {stats['total_likes']}, æ€»ä¸‹è½½: {stats['total_downloads']}")
        return author_stats
        
    except Exception as e:
        logger.error(f"âŒ è·å–ä½œè€… {author['author_name']} ç»Ÿè®¡æ•°æ®æ—¶å‡ºé”™: {e}")
        return {
            'user_id': author['user_id'],
            'author_name': author['author_name'],
            'profile_url': author['profile_url'],
            'total_actions': 0,
            'total_likes': 0,
            'total_downloads': 0,
            'total_pages': 0,
            'avg_likes': 0,
            'avg_downloads': 0,
            'extraction_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'error': str(e)
        }


def save_authors_stats_to_csv(authors_stats: List[Dict[str, any]], logger: logging.Logger):
    """
    å°†æ‰€æœ‰ä½œè€…çš„ç»Ÿè®¡æ•°æ®ä¿å­˜åˆ°CSVæ–‡ä»¶
    
    Args:
        authors_stats (List[Dict[str, any]]): ä½œè€…ç»Ÿè®¡æ•°æ®åˆ—è¡¨
        logger (logging.Logger): æ—¥å¿—è®°å½•å™¨
    """
    try:
        if not authors_stats:
            logger.warning("âŒ æ²¡æœ‰ä½œè€…ç»Ÿè®¡æ•°æ®å¯ä¿å­˜")
            return
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(authors_stats)
        
        # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåº
        columns_order = [
            'author_name', 'user_id', 'profile_url', 'total_actions', 
            'total_likes', 'total_downloads', 'avg_likes', 'avg_downloads', 
            'total_pages', 'extraction_time'
        ]
        
        # åªåŒ…å«å­˜åœ¨çš„åˆ—
        existing_columns = [col for col in columns_order if col in df.columns]
        df = df[existing_columns]
        
        # ä¿å­˜ä¸ºCSV - ä½¿ç”¨ç»Ÿä¸€çš„æ–‡ä»¶å
        csv_filename = "recommended_authors_stats.csv"
        df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
        
        logger.info(f"âœ… æ‰€æœ‰ä½œè€…ç»Ÿè®¡æ•°æ®å·²ä¿å­˜åˆ°: {csv_filename}")
        logger.info(f"ğŸ“Š å…±ç»Ÿè®¡äº† {len(authors_stats)} ä¸ªä½œè€…")
        
        # æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
        if len(authors_stats) > 0:
            total_actions = sum(author.get('total_actions', 0) for author in authors_stats)
            total_likes = sum(author.get('total_likes', 0) for author in authors_stats)
            total_downloads = sum(author.get('total_downloads', 0) for author in authors_stats)
            
            logger.info("=" * 60)
            logger.info("ğŸ“Š æ¨èä½œè€…ç»Ÿè®¡æ‘˜è¦")
            logger.info("=" * 60)
            logger.info(f"ä½œè€…æ€»æ•°: {len(authors_stats)}")
            logger.info(f"æ€»åŠ¨ä½œæ•°: {total_actions}")
            logger.info(f"æ€»ç‚¹èµæ•°: {total_likes}")
            logger.info(f"æ€»ä¸‹è½½æ•°: {total_downloads}")
            
            # æ˜¾ç¤ºç‚¹èµæ•°æœ€é«˜çš„ä½œè€…
            top_authors = sorted(authors_stats, key=lambda x: x.get('total_likes', 0), reverse=True)[:5]
            logger.info("\nğŸ† ç‚¹èµæ•°æœ€é«˜çš„ä½œè€…:")
            for i, author in enumerate(top_authors, 1):
                logger.info(f"  {i}. {author.get('author_name', 'Unknown')} - {author.get('total_likes', 0)} ç‚¹èµ")
        
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    debug_mode = '--debug' in sys.argv
    if '--debug' in sys.argv:
        sys.argv.remove('--debug')
    
    # è®¾ç½®æ—¥å¿—è®°å½•å™¨
    logger = setup_logger(debug=debug_mode)
    
    logger.info("=" * 60)
    logger.info("GetQuickeræ¨èé¡µé¢ä½œè€…ç»Ÿè®¡å·¥å…·")
    logger.info("=" * 60)
    
    logger.debug(f"è°ƒè¯•æ¨¡å¼: {'å¯ç”¨' if debug_mode else 'ç¦ç”¨'}")
    logger.info("-" * 60)
    
    # ç¬¬ä¸€æ­¥ï¼šè·å–æ¨èé¡µé¢å†…å®¹
    logger.info("æ­¥éª¤1: è·å–æ¨èé¡µé¢...")
    html_content = get_recommended_page_content(logger)
    
    if not html_content:
        logger.error("âŒ æ— æ³•è·å–æ¨èé¡µé¢å†…å®¹ï¼Œç¨‹åºé€€å‡º")
        return
    
    logger.info("-" * 60)
    
    # ç¬¬äºŒæ­¥ï¼šæå–ä½œè€…ä¿¡æ¯
    logger.info("æ­¥éª¤2: æå–ä½œè€…ä¿¡æ¯...")
    authors = extract_authors_from_recommended(html_content, logger)
    
    if not authors:
        logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•ä½œè€…ä¿¡æ¯ï¼Œç¨‹åºé€€å‡º")
        return
    
    logger.info("-" * 60)
    
    # ç¬¬ä¸‰æ­¥ï¼šè·å–æ¯ä¸ªä½œè€…çš„ç»Ÿè®¡æ•°æ®
    logger.info("æ­¥éª¤3: è·å–ä½œè€…ç»Ÿè®¡æ•°æ®...")
    authors_stats = []
    
    for i, author in enumerate(authors, 1):
        logger.info(f"è¿›åº¦: {i}/{len(authors)}")
        author_stats = get_author_stats(author, logger)
        authors_stats.append(author_stats)
        
        # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
        if i < len(authors):
            logger.debug("ç­‰å¾…1ç§’...")
            time.sleep(1)
    
    logger.info("-" * 60)
    
    # ç¬¬å››æ­¥ï¼šä¿å­˜ç»“æœåˆ°CSV
    logger.info("æ­¥éª¤4: ä¿å­˜ç»Ÿè®¡æ•°æ®...")
    save_authors_stats_to_csv(authors_stats, logger)
    
    logger.info("-" * 60)
    logger.info("ğŸ‰ ä»»åŠ¡å®Œæˆï¼")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
